{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/suhaibbasir/Documents/CS/MSc/Thesis/.conda/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  101,  5812, 27920,  6962,  2024,  3128,  2000,  1996, 18951,  1997,\n",
       "          6239,  1012,  2027,  2024,  2641,  2000,  2022,  2028,  1997,  1996,\n",
       "          2087,  9414,  4176,  1012, 23526,  2015,  2024,  1996,  2922, 25662,\n",
       "          2015,  1999,  1996,  2088,  1012,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ( \n",
    "    \"Organgutans are native to the rainforest of Indonesia.\"\n",
    "    \"They are considered to be one of the most intelligent animals.\"\n",
    "    \"Gorillas are the largest primates in the world.\"\n",
    ")\n",
    "\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'organ',\n",
       " '##gut',\n",
       " '##ans',\n",
       " 'are',\n",
       " 'native',\n",
       " 'to',\n",
       " 'the',\n",
       " 'rainforest',\n",
       " 'of',\n",
       " 'indonesia',\n",
       " '.',\n",
       " 'they',\n",
       " 'are',\n",
       " 'considered',\n",
       " 'to',\n",
       " 'be',\n",
       " 'one',\n",
       " 'of',\n",
       " 'the',\n",
       " 'most',\n",
       " 'intelligent',\n",
       " 'animals',\n",
       " '.',\n",
       " 'gorilla',\n",
       " '##s',\n",
       " 'are',\n",
       " 'the',\n",
       " 'largest',\n",
       " 'primate',\n",
       " '##s',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"naver/splade-cocondenser-ensembledistil\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"naver/splade-cocondenser-ensembledistil\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    \"The Amazon Rainforest, also known as the Amazon Jungle, is a vast tropical rainforest in South America, encompassing an area of approximately 5.5 million square kilometers. It spans across nine countries, including Brazil, Peru, Colombia, and Venezuela, with Brazil containing the largest portion. The Amazon Rainforest is renowned for its unparalleled biodiversity, housing over 400 billion individual trees representing around 16,000 species. It is home to an estimated 10% of all known species on Earth, including iconic wildlife such as jaguars, sloths, toucans, and pink river dolphins. Additionally, the Amazon River, the second longest river in the world, flows through this region, supporting a complex and interdependent ecosystem. The rainforest plays a crucial role in regulating the global climate by absorbing vast amounts of carbon dioxide and producing oxygen. However, it faces significant threats from deforestation, logging, mining, and agriculture, which have led to substantial habitat loss and environmental degradation. Efforts are underway to protect and conserve this critical natural resource through international cooperation, sustainable practices, and indigenous stewardship.\"\n",
    ")\n",
    "text2 = (\n",
    "    \"The Amazon River, flowing through the Amazon Rainforest, is the second longest river in the world. It supports a diverse ecosystem and is crucial for the rainforest's biodiversity.\"\n",
    ")\n",
    "text3 = (\n",
    "    \"The Great Wall of China is an ancient series of walls and fortifications, totaling more than 13,000 miles in length, located in northern China. It was built to protect against invasions and raids.\"\n",
    ")\n",
    "text4 = (\n",
    "    \"Amazon is founded by Jeff Bezos in 1994. It started as an online bookstore and has since expanded into various industries, including e-commerce, cloud computing, and entertainment.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "output = model(**tokens)\n",
    "\n",
    "tokens2 = tokenizer(text2, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "output2 = model(**tokens2)\n",
    "\n",
    "tokens3 = tokenizer(text3, return_tensors=\"pt\")\n",
    "output3 = model(**tokens3)\n",
    "\n",
    "tokens4 = tokenizer(text4, return_tensors=\"pt\")\n",
    "output4 = model(**tokens4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "def get_max_logits(output, tokens):\n",
    "    return torch.max(\n",
    "        torch.log(\n",
    "            1 + torch.relu(output.logits)\n",
    "        ) * tokens.attention_mask.unsqueeze(-1),\n",
    "        dim=1)[0].squeeze().detach().cpu().numpy()\n",
    "\n",
    "vec = get_max_logits(output, tokens)\n",
    "vec2 = get_max_logits(output2, tokens2)\n",
    "vec3 = get_max_logits(output3, tokens3)\n",
    "vec4 = get_max_logits(output4, tokens4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n",
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(vec)\n",
    "print(vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.99999994, 0.48202875, 0.09869803, 0.17222472],\n",
       "       [0.48202875, 1.        , 0.05827487, 0.26472694],\n",
       "       [0.09869803, 0.05827487, 1.        , 0.00996668],\n",
       "       [0.17222472, 0.26472694, 0.00996668, 1.        ]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similarity([vec, vec2, vec3, vec4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2token = {\n",
    "    idx: token for token, idx in tokenizer.get_vocab().items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'squeeze'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cols \u001b[38;5;241m=\u001b[39m \u001b[43mvec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnonzero\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      2\u001b[0m cols2 \u001b[38;5;241m=\u001b[39m vec2\u001b[38;5;241m.\u001b[39mnonzero()\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m      3\u001b[0m cols3 \u001b[38;5;241m=\u001b[39m vec3\u001b[38;5;241m.\u001b[39mnonzero()\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'squeeze'"
     ]
    }
   ],
   "source": [
    "cols = vec.nonzero().squeeze().cpu().numpy()\n",
    "cols2 = vec2.nonzero().squeeze().cpu().numpy()\n",
    "cols3 = vec3.nonzero().squeeze().cpu().numpy()\n",
    "print(len(cols))\n",
    "\n",
    "weights = vec[cols].cpu().tolist()\n",
    "weights2 = vec2[cols2].cpu().tolist()\n",
    "weights3 = vec3[cols3].cpu().tolist()\n",
    "\n",
    "sparse_dict = dict(zip(cols, weights))\n",
    "sparse_dict2 = dict(zip(cols2, weights2))\n",
    "sparse_dict3 = dict(zip(cols3, weights3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m sparse_dict_tokens \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m----> 2\u001b[0m     idx2token[idx]: \u001b[38;5;28mround\u001b[39m(weight, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m idx, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[43mcols\u001b[49m, weights)\n\u001b[1;32m      3\u001b[0m }\n\u001b[1;32m      4\u001b[0m sparse_dict_tokens2 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     idx2token[idx]: \u001b[38;5;28mround\u001b[39m(weight, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m idx, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(cols2, weights2)\n\u001b[1;32m      6\u001b[0m }\n\u001b[1;32m      7\u001b[0m sparse_dict_tokens3 \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      8\u001b[0m     idx2token[idx]: \u001b[38;5;28mround\u001b[39m(weight, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m idx, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(cols3, weights3)\n\u001b[1;32m      9\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cols' is not defined"
     ]
    }
   ],
   "source": [
    "sparse_dict_tokens = {\n",
    "    idx2token[idx]: round(weight, 2) for idx, weight in zip(cols, weights)\n",
    "}\n",
    "sparse_dict_tokens2 = {\n",
    "    idx2token[idx]: round(weight, 2) for idx, weight in zip(cols2, weights2)\n",
    "}\n",
    "sparse_dict_tokens3 = {\n",
    "    idx2token[idx]: round(weight, 2) for idx, weight in zip(cols3, weights3)\n",
    "}\n",
    "\n",
    "sparse_dict_tokens = {\n",
    "    k: v for k, v in sorted(\n",
    "        sparse_dict_tokens.items(),\n",
    "        key=lambda item: item[1],\n",
    "        reverse=True\n",
    "    )\n",
    "}\n",
    "sparse_dict_tokens\n",
    "\n",
    "sparse_dict_tokens2 = {\n",
    "    k: v for k, v in sorted(\n",
    "        sparse_dict_tokens2.items(),\n",
    "        key=lambda item: item[1],\n",
    "        reverse=True\n",
    "    )\n",
    "}\n",
    "sparse_dict_tokens2\n",
    "\n",
    "sparse_dict_tokens3 = {\n",
    "    k: v for k, v in sorted(\n",
    "        sparse_dict_tokens3.items(),\n",
    "        key=lambda item: item[1],\n",
    "        reverse=True\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30522,)\n",
      "(30522,)\n"
     ]
    }
   ],
   "source": [
    "print(vec.shape)\n",
    "print(vec2.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "boost",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
